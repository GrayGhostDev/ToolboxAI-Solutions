#!/usr/bin/env python3
"""
Performance Test Runner

Runs comprehensive performance optimization tests and generates reports.
Designed to validate P95 latency <150ms target and throughput improvements.
"""

import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path


def run_test_suite(test_path: str, test_name: str, markers: str = None) -> dict:
    """Run a test suite and collect results."""
    print(f"\n{'='*60}")
    print(f"Running {test_name}")
    print(f"{'='*60}")

    cmd = [
        sys.executable,
        "-m",
        "pytest",
        test_path,
        "-v",
        "--tb=short",
        "--json-report",
        f"--json-report-file=test_results_{test_name.lower().replace(' ', '_')}.json",
    ]

    if markers:
        cmd.extend(["-m", markers])

    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True)
    end_time = time.time()

    return {
        "test_name": test_name,
        "duration": end_time - start_time,
        "return_code": result.returncode,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "passed": result.returncode == 0,
    }


def generate_performance_report(results: list):
    """Generate a performance test report."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"performance_test_report_{timestamp}.md"

    total_tests = len(results)
    passed_tests = sum(1 for r in results if r["passed"])
    total_time = sum(r["duration"] for r in results)

    report = f"""# Performance Optimization Test Report

**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Total Test Suites**: {total_tests}
**Passed Test Suites**: {passed_tests}
**Failed Test Suites**: {total_tests - passed_tests}
**Total Execution Time**: {total_time:.2f} seconds

## Executive Summary

This report covers comprehensive testing of the performance optimization modules:
- **Redis Cache Layer**: Connection pooling, serialization, compression, statistics tracking
- **Database Optimization**: Connection pooling, query caching, prepared statements
- **Pusher Optimization**: Connection reuse, event batching, rate limiting
- **Integration Testing**: End-to-end optimization workflows
- **Benchmark Testing**: Performance validation against P95 latency <150ms target

## Test Suite Results

"""

    for result in results:
        status = "‚úÖ PASSED" if result["passed"] else "‚ùå FAILED"
        report += f"### {result['test_name']} - {status}\n\n"
        report += f"- **Duration**: {result['duration']:.2f} seconds\n"
        report += f"- **Return Code**: {result['return_code']}\n"

        if not result["passed"]:
            report += f"\n**Error Details**:\n```\n{result['stderr']}\n```\n"

        report += "\n"

    report += f"""## Performance Targets

The performance optimization system is designed to achieve:

- **P95 Latency**: <150ms for end-to-end requests
- **P99 Latency**: <200ms for end-to-end requests
- **Cache Hit Rate**: >80% for frequently accessed data
- **Database Query Performance**: <100ms P95 for simple queries
- **Pusher Event Delivery**: <50ms P95 latency
- **Throughput**: >500 requests/second under normal load

## Code Coverage

The test suites aim for >95% code coverage across all optimization modules:

- **Cache Optimization**: All cache operations, serialization, compression, statistics
- **Database Optimization**: Connection pooling, query caching, prepared statements
- **Pusher Optimization**: Connection pooling, event batching, rate limiting
- **Integration**: Cross-component workflows and error handling

## Recommendations

### If Tests Are Passing ‚úÖ
- Performance optimizations are working as expected
- System should meet P95 latency targets
- Continue monitoring performance metrics in production

### If Tests Are Failing ‚ùå
1. **Check Dependencies**: Ensure all required packages are installed
2. **Environment Setup**: Verify test environment configuration
3. **Resource Availability**: Check system resources (CPU, memory)
4. **Mock Configuration**: Ensure mocks are properly configured
5. **Run Individual Tests**: Isolate failing components

## Next Steps

1. **Deploy to Staging**: Run performance tests in staging environment
2. **Load Testing**: Conduct load testing with realistic traffic patterns
3. **Monitoring Setup**: Implement performance monitoring in production
4. **Continuous Testing**: Add performance tests to CI/CD pipeline

---

*Generated by ToolboxAI Performance Test Suite*
"""

    with open(report_file, "w") as f:
        f.write(report)

    print(f"\nüìä Performance report generated: {report_file}")
    return report_file


def main():
    """Main test execution function."""
    print("üöÄ Starting Performance Optimization Test Suite")
    print(f"Working directory: {os.getcwd()}")

    # Change to project root
    project_root = Path(__file__).parent.parent.parent
    os.chdir(project_root)

    test_suites = [
        {
            "path": "tests/unit/backend/test_cache_optimization.py",
            "name": "Cache Optimization Unit Tests",
            "markers": "unit",
        },
        {
            "path": "tests/unit/backend/test_db_optimization.py",
            "name": "Database Optimization Unit Tests",
            "markers": "unit",
        },
        {
            "path": "tests/unit/backend/test_pusher_optimization.py",
            "name": "Pusher Optimization Unit Tests",
            "markers": "unit",
        },
        {
            "path": "tests/integration/test_performance_integration.py",
            "name": "Performance Integration Tests",
            "markers": "integration",
        },
        {
            "path": "tests/performance/test_benchmark_suite.py",
            "name": "Performance Benchmark Tests",
            "markers": "performance",
        },
    ]

    results = []

    for suite in test_suites:
        if not Path(suite["path"]).exists():
            print(f"‚ö†Ô∏è  Test file not found: {suite['path']}")
            continue

        try:
            result = run_test_suite(suite["path"], suite["name"], suite.get("markers"))
            results.append(result)

            if result["passed"]:
                print(f"‚úÖ {suite['name']} - PASSED")
            else:
                print(f"‚ùå {suite['name']} - FAILED")
                print(
                    "Error output:",
                    (
                        result["stderr"][:200] + "..."
                        if len(result["stderr"]) > 200
                        else result["stderr"]
                    ),
                )

        except Exception as e:
            print(f"‚ùå Error running {suite['name']}: {e}")
            results.append(
                {
                    "test_name": suite["name"],
                    "duration": 0,
                    "return_code": -1,
                    "stdout": "",
                    "stderr": str(e),
                    "passed": False,
                }
            )

    # Generate report
    report_file = generate_performance_report(results)

    # Summary
    passed_count = sum(1 for r in results if r["passed"])
    total_count = len(results)

    print(f"\n{'='*60}")
    print(f"PERFORMANCE TEST SUMMARY")
    print(f"{'='*60}")
    print(f"Total Test Suites: {total_count}")
    print(f"Passed: {passed_count}")
    print(f"Failed: {total_count - passed_count}")
    print(f"Success Rate: {passed_count/total_count*100:.1f}%")
    print(f"Report: {report_file}")

    if passed_count == total_count:
        print("\nüéâ All performance optimization tests passed!")
        print("‚úÖ System should meet P95 latency <150ms target")
        sys.exit(0)
    else:
        print(f"\n‚ö†Ô∏è  {total_count - passed_count} test suite(s) failed")
        print("‚ùå Performance optimizations may not meet targets")
        sys.exit(1)


if __name__ == "__main__":
    main()
