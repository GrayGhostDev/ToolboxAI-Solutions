name: Test Suite

on:
  push:
    branches: [main, develop, 'feature/*', 'hotfix/*']
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 2 * * *'  # Run nightly tests at 2 AM UTC
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PYTEST_ADDOPTS: '--strict-markers --strict-config'

jobs:
  test-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.11', '3.12']
        include:
          - os: ubuntu-latest
            python-version: '3.11'
            coverage: true
          - os: ubuntu-latest
            python-version: '3.12'
            experimental: true
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            requirements-dev.txt
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: |
            package-lock.json
            API/Dashboard/package-lock.json
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Install Node.js dependencies
        run: |
          npm ci
          cd API/Dashboard && npm ci
      
      - name: Create test environment
        shell: bash
        run: |
          cp .env.example .env
          cat >> .env << EOF
          DATABASE_URL=postgresql://postgres:postgres@localhost:5432/testdb
          REDIS_URL=redis://localhost:6379
          OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
          TEST_MODE=true
          LOG_LEVEL=DEBUG
          EOF
      
      - name: Initialize test database
        run: |
          python -c "
          import asyncio
          from server.database import create_tables
          asyncio.run(create_tables())
          "
      
      - name: Run unit tests
        if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == '' }}
        run: |
          pytest tests/unit/ \
            --verbose \
            --tb=short \
            --maxfail=10 \
            --numprocesses auto \
            --dist worksteal \
            ${{ matrix.coverage && '--cov=server --cov=agents --cov=mcp --cov=sparc --cov=swarm --cov=coordinators' || '' }} \
            ${{ matrix.coverage && '--cov-report=xml --cov-report=term-missing --cov-report=html' || '' }} \
            --junitxml=unit-test-results.xml
      
      - name: Run integration tests
        if: ${{ (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == '') && matrix.os == 'ubuntu-latest' }}
        timeout-minutes: 15
        run: |
          pytest tests/integration/ \
            --verbose \
            --tb=short \
            --timeout=300 \
            --junitxml=integration-test-results.xml
      
      - name: Run API tests with Dashboard
        if: ${{ matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11' }}
        run: |
          # Start background servers
          python server/main.py &
          API_PID=$!
          sleep 10
          
          # Test API endpoints
          curl -f http://localhost:8008/health || exit 1
          curl -f http://localhost:8008/docs || exit 1
          
          # Run Dashboard tests
          cd API/Dashboard
          npm test -- --watchAll=false --coverage --testResultsProcessor=jest-junit
          
          # Cleanup
          kill $API_PID
        env:
          CI: true
      
      - name: Upload coverage to Codecov
        if: ${{ matrix.coverage && success() }}
        uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage.xml,./API/Dashboard/coverage/lcov.info
          flags: ${{ matrix.os }}-${{ matrix.python-version }}
          name: codecov-${{ matrix.os }}-${{ matrix.python-version }}
          fail_ci_if_error: false
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
          path: |
            *-test-results.xml
            htmlcov/
            API/Dashboard/coverage/
            pytest.log
          retention-days: 30

  e2e-tests:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'e2e' || github.event.inputs.test_type == '' }}
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install playwright pytest-playwright
          npm ci
          cd API/Dashboard && npm ci
          playwright install --with-deps chromium
      
      - name: Build Dashboard
        run: |
          cd API/Dashboard
          npm run build
      
      - name: Start application stack
        run: |
          cp .env.example .env
          echo "DATABASE_URL=postgresql://postgres:postgres@localhost:5432/testdb" >> .env
          echo "REDIS_URL=redis://localhost:6379" >> .env
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env
          
          # Start services
          python server/main.py &
          python server/roblox_server.py &
          python mcp/server.py &
          
          cd API/Dashboard
          npm start &
          
          # Wait for services to be ready
          sleep 30
      
      - name: Run E2E tests
        run: |
          pytest tests/e2e/ \
            --verbose \
            --tb=short \
            --browser chromium \
            --video=on-first-retry \
            --screenshot=only-on-failure \
            --junitxml=e2e-test-results.xml
      
      - name: Upload E2E artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-artifacts
          path: |
            test-results/
            e2e-test-results.xml
          retention-days: 14

  performance-tests:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' || github.event_name == 'schedule' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark locust
      
      - name: Run performance benchmarks
        run: |
          pytest tests/performance/ \
            --verbose \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-compare-fail=mean:10% \
            --junitxml=performance-test-results.xml
      
      - name: Run load tests
        run: |
          cp .env.example .env
          python server/main.py &
          sleep 10
          
          locust -f tests/performance/load_test.py \
            --host=http://localhost:8008 \
            --users=10 \
            --spawn-rate=2 \
            --run-time=2m \
            --html=load-test-report.html \
            --headless
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-results
          path: |
            benchmark-results.json
            load-test-report.html
            performance-test-results.xml

  security-tests:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'security' || github.event_name == 'schedule' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install security tools
        run: |
          pip install bandit safety semgrep pytest
          npm install -g audit-ci
      
      - name: Run Bandit security scan
        run: |
          bandit -r server/ agents/ mcp/ sparc/ swarm/ coordinators/ \
            -f json -o bandit-report.json \
            -ll -x "*/tests/*"
        continue-on-error: true
      
      - name: Run Safety dependency scan
        run: |
          safety check --json --output safety-report.json
        continue-on-error: true
      
      - name: Run Semgrep static analysis
        run: |
          semgrep --config=auto --json --output=semgrep-report.json \
            server/ agents/ mcp/ sparc/ swarm/ coordinators/
        continue-on-error: true
      
      - name: Run npm audit
        run: |
          cd API/Dashboard
          audit-ci --config audit-ci.json
        continue-on-error: true
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            semgrep-report.json
            API/Dashboard/audit-results.json

  test-summary:
    runs-on: ubuntu-latest
    needs: [test-matrix, e2e-tests, performance-tests, security-tests]
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3
      
      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            **/unit-test-results.xml
            **/integration-test-results.xml
            **/e2e-test-results.xml
            **/performance-test-results.xml
          check_name: "Test Results"
          comment_title: "Test Results Summary"
      
      - name: Create test summary
        if: always()
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f test-matrix/unit-test-results.xml ]; then
            echo "✅ Unit tests completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Unit tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f e2e-artifacts/e2e-test-results.xml ]; then
            echo "✅ E2E tests completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ E2E tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f performance-results/performance-test-results.xml ]; then
            echo "✅ Performance tests completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Performance tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f security-reports/bandit-report.json ]; then
            echo "✅ Security scans completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Security scans failed" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Notify on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: 'Test suite failed! Check the workflow for details.'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  cleanup-test-data:
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always()
    steps:
      - name: Clean up test artifacts
        run: |
          echo "Cleaning up test data..."
          # Add any cleanup scripts here
          echo "Test cleanup completed"